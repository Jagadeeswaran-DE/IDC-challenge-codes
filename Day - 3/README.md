# Day 03 â€“ PySpark Transformations Deep Dive ğŸš€

Part of the **Databricks 14 Days AI Challenge** by Indian Data Club.

This day focused on going beyond basic Spark syntax and working with PySpark the way itâ€™s actually used in real data engineering pipelines.

---

## ğŸ§  What This Day Was About

The goal of Day 3 was to take a **real e-commerce clickstream dataset** and transform it into something analytics- and business-ready using PySpark.

Instead of toy examples, the work was done on a **full-scale dataset** with real-world complexity:
- Multiple event types
- Missing values
- Repeated records
- User and product behavior over time

---

## ğŸ“Š Dataset Overview

The dataset represents user interaction events in an e-commerce platform.

**Schema highlights:**
- `event_time` â€“ Timestamp of the event  
- `event_type` â€“ view / cart / purchase  
- `product_id` â€“ Product identifier  
- `category_id`, `category_code` â€“ Product categorization  
- `brand` â€“ Product brand  
- `price` â€“ Product price  
- `user_id` â€“ User identifier  
- `user_session` â€“ Session identifier  

This is a classic **event-level fact table** used in analytics engineering.

---

## ğŸ› ï¸ What I Implemented

### 1ï¸âƒ£ Data Loading
- Loaded the full CSV dataset into Databricks using PySpark
- Verified schema and data types
- Worked directly on large-scale data (not samples)

---

### 2ï¸âƒ£ Event Segmentation
Split the raw events into logical subsets:
- Views
- Carts
- Purchases

This made downstream joins and funnel analysis much cleaner.

---

### 3ï¸âƒ£ Joins (Core Focus ğŸ”¥)

Implemented **all major join types** using real business logic:

- **INNER JOIN**  
  Used to find strict matches (e.g., users who both viewed and purchased the same product)

- **LEFT JOIN**  
  Preserved all events while enriching with related data

- **RIGHT JOIN**  
  Identified entities that exist but have missing activity

- **FULL OUTER JOIN**  
  Used for data audits and reconciliation between datasets

Also implemented:
- Conditional joins
- Self joins (event-to-event comparisons)

---

### 4ï¸âƒ£ Window Functions
Used Spark window functions to move from row-level data to sequence-based insights.

Examples:
- Ranking user events by time
- Tracking user activity over sessions
- Calculating running metrics without collapsing rows

This is a key concept for analytics and behavioral analysis.

---

### 5ï¸âƒ£ Derived Metrics & Features
Created analytics-ready outputs such as:
- View â†’ Purchase conversion counts
- Category-level performance metrics
- Aggregated event statistics

Turned raw logs into **decision-ready data**.

---

## ğŸš€ Tech Stack

- Apache Spark
- PySpark
- Databricks (Community Edition)
- Python

---

## ğŸ“Œ Key Learnings

- Joins are not just SQL operations â€” they represent business questions
- Window functions are essential for behavioral analytics
- Event data needs structure before it becomes useful
- PySpark transformations scale only when logic is clean

---

## ğŸ”— Repository Structure

